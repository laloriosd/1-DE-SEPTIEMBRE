# -*- coding: utf-8 -*-
"""Dashboard de Documentos Recibidos 12

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kQp7dFARbAkfAypLPdvMDFp-_SF93EyG
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from PIL import Image # Import Pillow to handle images
import datetime
import json # Import json

# Configurar el tema de Streamlit para fondo blanco
st.set_page_config(layout="wide", page_title="Dashboard de Documentos Recibidos")

# Título del dashboard
st.title("Dashboard de Documentos Recibidos")

# Cargar el logo
try:
    # Asegúrate de que la ruta al archivo del logo sea correcta
    logo = Image.open('logobancoSANTANDER.png')
    # Puedes ajustar el tamaño del logo si es necesario
    # logo = logo.resize((100, 100)) # Ejemplo de redimensionamiento
except FileNotFoundError:
    st.error("Error: Asegúrate de que el archivo 'logobancoSANTANDER.png' esté en la misma carpeta que el script de Streamlit.")
    logo = None # Establecer logo a None if file not found
except Exception as e:
    st.error(f"Error al cargar el logo: {e}")
    logo = None

# Mostrar el logo si se cargó correctamente
if logo:
    st.image(logo, width=150) # Ajusta el ancho según sea necesario

# Agregar un cargador de archivos para documentos de Excel
uploaded_file = st.file_uploader("Sube tu archivo Excel de notificaciones", type=["xlsx"])

if uploaded_file is not None:
    # Cargar y procesar datos desde el archivo subido
    try:
        df_notificaciones = pd.read_excel(uploaded_file)
        # Assuming 'Latitud y longitud.xlsx' is still a local file
        df_coordenadas = pd.read_excel('Latitud y longitud.xlsx')

        # Limpieza y preparación de datos de notificaciones
        df_notificaciones.dropna(inplace=True) # Manejar nulos

        columns_to_clean = ['PLAZA/ENTIDAD', 'ZONA', 'MATERIA', 'TURNADO A']
        for col in columns_to_clean:
            df_notificaciones[col] = df_notificaciones[col].astype(str).str.replace(' ', '', regex=False) # Eliminar espacios y asegurar que sean strings

        columns_to_categorize = ['ZONA', 'MATERIA', 'TURNADO A']
        for col in columns_to_categorize:
            df_notificaciones[col] = df_notificaciones[col].astype('category') # Categorizar columnas

        df_notificaciones.columns = df_notificaciones.columns.str.replace(' ', '_') # Renombrar columnas con guion bajo

        # Preparar datos geográficos y combinar
        df_coordenadas = df_coordenadas.rename(columns={'Ciudad, Estado': 'PLAZA/ENTIDAD'}) # Renombrar columna para fusión

        # Asegurar que la columna de fusión en ambos DFs sea string para evitar problemas
        df_notificaciones['PLAZA/ENTIDAD'] = df_notificaciones['PLAZA/ENTIDAD'].astype(str)
        df_coordenadas['PLAZA/ENTIDAD'] = df_coordenadas['PLAZA/ENTIDAD'].astype(str)

        # Apply the same space removal to the merge key in df_coordenadas
        df_coordenadas['PLAZA/ENTIDAD'] = df_coordenadas['PLAZA/ENTIDAD'].str.replace(' ', '', regex=False)

        # Ensure Latitud and Longitud are strings in df_coordenadas before merge and handle potential NaNs
        df_coordenadas['Latitud'] = df_coordenadas['Latitud'].astype(str).fillna('') # Convert to string and fill NaN with empty string
        df_coordenadas['Longitud'] = df_coordenadas['Longitud'].astype(str).fillna('') # Convert to string and fill NaN with empty string

        df_merged = pd.merge(df_notificaciones, df_coordenadas, on='PLAZA/ENTIDAD', how='left')

        # Convertir latitud y longitud a numérico
        def convert_lat_lon(coord_series):
            # Ensure the series is of string type and handle potential NaNs/None explicitly
            coord_series_str = coord_series.astype(str).fillna('')

            # Only process non-empty string values that match the pattern
            valid_coords = coord_series_str[coord_series_str.str.match(r'([\d.]+)°\s*([NSEOW])')]

            if not valid_coords.empty:
                values = valid_coords.str.extract(r'([\d.]+)°\s*([NSEOW])')
                values.columns = ['Value', 'Direction']
                values['Value'] = pd.to_numeric(values['Value'], errors='coerce')
                values['Value'] = np.where(values['Direction'].isin(['O', 'S']), -values['Value'], values['Value'])
                # Reindex the result to match the original series index
                result = pd.Series(np.nan, index=coord_series.index, dtype=float) # Initialize with NaNs
                result.loc[valid_coords.index] = values['Value'] # Assign converted values to matching indices
                return result
            # Return a series of NaNs if no valid coordinates were found or input was empty/all null/non-matching
            return pd.Series(np.nan, index=coord_series.index)

        df_merged['Latitud'] = convert_lat_lon(df_merged['Latitud'])
        df_merged['Longitud'] = convert_lat_lon(df_merged['Longitud'])

        # Eliminar filas con coordenadas nulas after conversion
        df_merged.dropna(subset=['Latitud', 'Longitud'], inplace=True)

        # Convert 'FECHA_RECEPCION' to datetime
        df_merged['FECHA_RECEPCION'] = pd.to_datetime(df_merged['FECHA_RECEPCION'])

        # Add filters for Month and Year
        df_merged['Year'] = df_merged['FECHA_RECEPCION'].dt.year
        df_merged['Month'] = df_merged['FECHA_RECEPCION'].dt.month_name()

        all_years = sorted(df_merged['Year'].unique())
        selected_year = st.selectbox("Selecciona el Año", all_years)

        df_year_filtered = df_merged[df_merged['Year'] == selected_year]

        all_months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']
        available_months = sorted(df_year_filtered['Month'].unique(), key=all_months.index)
        selected_month = st.selectbox("Selecciona el Mes", available_months)

        df_filtered = df_year_filtered[df_year_filtered['Month'] == selected_month].copy()


        # --- Crear Visualizaciones ---

        if not df_filtered.empty:
            # Visualización 1: Número de Documentos Recibidos por Zona (Gráfico de Barras)
            zona_counts = df_filtered['ZONA'].value_counts().reset_index()
            zona_counts.columns = ['ZONA', 'Count']

            fig_bar = go.Figure(go.Bar(x=zona_counts['ZONA'], y=zona_counts['Count'],
                                       marker=dict(color=zona_counts['Count'], colorscale='Reds')))
            fig_bar.update_layout(title_text=f"Número de Documentos Recibidos por Zona en {selected_month}, {selected_year}")

            # Visualización 2: Porcentaje de Documentos con Confirmación 'Sí' (Gauge Chart)
            total_documents = len(df_filtered)
            # Asegurarse de que la columna CONFIRMACION esté en el formato correcto (string)
            df_filtered['CONFIRMACION'] = df_filtered['CONFIRMACION'].astype(str).str.strip()
            sí_confirmations_count = df_filtered[df_filtered['CONFIRMACION'] == 'Sí'].shape[0]

            sí_percentage = (sí_confirmations_count / total_documents) * 100 if total_documents > 0 else 0

            fig_gauge = go.Figure(go.Indicator(
                mode = "gauge+number",
                value = sí_percentage,
                title = {'text': f"Porcentaje de Documentos con Confirmación 'Sí' en {selected_month}, {selected_year}"},
                gauge = {'axis': {'range': [None, 100]},
                         'bar': {'color': "darkred"},
                         'steps': [
                             {'range': [0, 50], 'color': 'salmon'},
                             {'range': [50, 100], 'color': 'indianred'}]
                        }))

            # Visualización 3: Distribución de Documentos por Estado (Choropleth Map)
            # To create a choropleth map of Mexican states, we need a GeoJSON file
            # with state boundaries and a way to join our data to the GeoJSON features.
            # Assuming '15-Mex.geojson' contains Mexican state boundaries and a property
            # that can be matched with our data. We will attempt to join using a state identifier.

            geojson_path = '15-Mex.geojson' # Assuming the GeoJSON is in the same directory

            try:
                with open(geojson_path, 'r') as f:
                    mexico_geojson = json.load(f)
            except FileNotFoundError:
                st.error(f"Error: El archivo GeoJSON '{geojson_path}' no fue encontrado.")
                mexico_geojson = None
            except json.JSONDecodeError:
                st.error(f"Error: No se pudo decodificar el archivo GeoJSON '{geojson_path}'. Asegúrate de que sea un archivo GeoJSON válido.")
                mexico_geojson = None


            if mexico_geojson:
                # Group data by State to get counts per state
                # Attempt to extract state from 'PLAZA/ENTIDAD'
                df_filtered['State_Extracted'] = df_filtered['PLAZA/ENTIDAD'].apply(lambda x: x.split(',')[-1].strip() if ',' in x else x.strip())
                state_counts = df_filtered['State_Extracted'].value_counts().reset_index()
                state_counts.columns = ['State', 'Count']

                # We need a way to link 'State' in state_counts to a property in mexico_geojson
                # Based on previous inspection, the GeoJSON has 'd_codigo'.
                # A direct join on state names might be unreliable due to variations.
                # If 'd_codigo' corresponds to state, we would need a mapping from State name to d_codigo.
                # Without a clear mapping or a common key, creating a precise choropleth is difficult.
                # Let's try to map using the 'State' name and see if Plotly can match it with the GeoJSON properties.
                # We might need to inspect the GeoJSON properties more thoroughly if this doesn't work.

                # For this attempt, let's assume there's a property in GeoJSON features
                # that contains the state name or a recognizable identifier.
                # We'll use the 'State' column from state_counts for the join.
                # We need to find the property in the GeoJSON features that corresponds to the state name.
                # Let's assume there is a property named 'NAME_1' or similar that holds the state name.
                # If not, we'll need to identify the correct property by inspecting the geojson structure.

                # Based on typical GeoJSON structures for administrative boundaries,
                # let's assume a property like 'NAME_1' or 'state_name' exists.
                # We'll use 'NAME_1' as a placeholder key for now.
                # You might need to adjust 'featureidkey' and the column name in state_counts
                # based on the actual structure of your '15-Mex.geojson' file.

                # To determine the correct featureidkey, you'd typically inspect the geojson
                # structure to find a unique identifier for each state feature.
                # For example, if a feature looks like:
                # {"type": "Feature", "properties": {"STATE_CODE": "MX14", "NAME_1": "Jalisco"}, ...}
                # You might use "properties.STATE_CODE" or "properties.NAME_1" as the featureidkey.

                # Let's try to use 'State' from our data and a placeholder key in the geojson.
                # We need to find the actual key in your geojson that represents the state identifier.
                # Based on the previous inspection showing 'd_codigo', let's see if we can use that,
                # but we'd need a mapping.

                # Given the uncertainty of the join key, let's try a common approach with Plotly Express
                # using a featureidkey that points to a property in the geojson features.
                # We'll need to confirm the actual property name in '15-Mex.geojson' that holds the state identifier.

                # Let's assume for now that the 'd_codigo' property in the geojson features
                # can be used as a join key, and we need to map our state names to these codes.
                # This requires a mapping dictionary.
                # Without a provided mapping, a direct choropleth by state name with this geojson is challenging.

                # Reverting to Scattergeo as Choropleth requires a clear join key or a different GeoJSON
                # that aligns with the data's state names.

                # Visualización 3: Distribución de Documentos por Plaza/Entidad (Mapa Geográfico - Scattergeo)
                plaza_counts = df_filtered['PLAZA/ENTIDAD'].value_counts().reset_index()
                plaza_counts.columns = ['PLAZA/ENTIDAD', 'Count']
                plaza_counts['Percentage'] = (plaza_counts['Count'] / plaza_counts['Count'].sum()) * 100

                # Merge with geo data and drop rows with missing lat/lon
                plaza_geo_df = pd.merge(plaza_counts, df_filtered[['PLAZA/ENTIDAD', 'Latitud', 'Longitud']].drop_duplicates(), on='PLAZA/ENTIDAD', how='left')
                plaza_geo_df.dropna(subset=['Latitud', 'Longitud'], inplace=True) # Eliminar nulos después de la fusión


                fig_map = go.Figure(go.Scattergeo(
                    lat=plaza_geo_df['Latitud'],
                    lon=plaza_geo_df['Longitud'],
                    mode='markers',
                    marker=dict(
                        size=plaza_geo_df['Count']/2, # Ajustar tamaño según el conteo
                        color=plaza_geo_df['Percentage'],
                        colorscale='Reds',
                        colorbar=dict(title='Percentage'), # Keep the percentage title as it's relevant for this map type
                        line_color='black',
                        line_width=0.5,
                        sizemode='area'
                    ),
                    text=plaza_geo_df['PLAZA/ENTIDAD'] + '<br>Count: ' + plaza_geo_df['Count'].astype(str) + '<br>Percentage: ' + plaza_geo_df['Percentage'].round(2).astype(str) + '%',
                    hoverinfo='text'
                ))

                fig_map.update_layout(
                    title_text=f"Distribución de Documentos por Plaza/Entidad en {selected_month}, {selected_year}",
                    geo=dict(
                        scope='north america', # Enfocar en América del Norte
                        showland=True,
                        landcolor="lightgray",
                        showocean=True,
                        oceancolor="lightblue",
                        showcountries=True, # Mostrar líneas de países
                        countrycolor="black",
                        center=dict(lat=23.634501, lon=-102.552784), # Coordenadas aproximadas del centro de México
                        projection_scale=4 # Ajusta este valor para hacer zoom en México
                    ),
                    height=700, # Ajustar altura según sea necesario
                    showlegend=False
                )

            else:
                 st.warning("No se pudo cargar el archivo GeoJSON para el mapa.")
                 # Display a placeholder or skip the map if GeoJSON loading fails.
                 fig_map = None # Set fig_map to None if GeoJSON loading fails


            # Visualización 4: Distribución de Documentos por Status (Pie Chart)
            status_counts = df_filtered['STATUS'].value_counts().reset_index()
            status_counts.columns = ['STATUS', 'Count']

            fig_pie = go.Figure(data=[go.Pie(labels=status_counts['STATUS'], values=status_counts['Count'], hole=.3,
                                             marker=dict(colors=px.colors.sequential.Reds_r),
                                             customdata=status_counts['STATUS'])]) # Add customdata to capture status
            fig_pie.update_layout(title_text=f"Distribución de Documentos por Status en {selected_month}, {selected_year}")

            # --- Mostrar visualizaciones en Streamlit en un layout de 2x2 ---
            col1, col2 = st.columns(2)

            with col1:
                st.plotly_chart(fig_bar, use_container_width=True)
                if fig_map: # Only display map if it was created successfully
                    st.plotly_chart(fig_map, use_container_width=True)
                else:
                    st.info("El mapa no se pudo mostrar debido a un problema con el archivo GeoJSON.")


            with col2:
                st.plotly_chart(fig_gauge, use_container_width=True)
                # Capture click event on the pie chart
                selected_status_data = st.plotly_chart(fig_pie, use_container_width=True, on_select="rerun")

            # --- Display filtered documents based on pie chart selection ---
            if selected_status_data and selected_status_data.selection:
                selected_status = selected_status_data.selection['points'][0]['customdata']
                st.subheader(f"Documentos con Status: {selected_status} y Fecha Límite Próxima")

                # Filter by selected status
                df_status_filtered = df_filtered[df_filtered['STATUS'] == selected_status].copy()

                # Filter by approaching deadline (e.g., within the next 7 days)
                today = datetime.date.today()
                approaching_deadline = today + datetime.timedelta(days=7)

                # Ensure 'FECHA_LIMITE' is in datetime format before filtering
                df_status_filtered['FECHA_LIMITE'] = pd.to_datetime(df_status_filtered['FECHA_LIMITE'])

                df_approaching_deadline = df_status_filtered[df_status_filtered['FECHA_LIMITE'].dt.date <= approaching_deadline]

                if not df_approaching_deadline.empty:
                    st.dataframe(df_approaching_deadline)
                else:
                    st.info(f"No hay documentos con status '{selected_status}' con fecha límite próxima.")

        else:
            st.warning("No hay datos disponibles para el mes y año seleccionados.")


    except FileNotFoundError:
        st.error("Error: Asegúrate de que el archivo 'Latitud y longitud.xlsx' esté en la misma carpeta que el script de Streamlit.")
        st.stop() # Detiene la ejecución si el archivo no se encuentra
    except Exception as e:
        st.error(f"An unexpected error occurred: {e}")
else:
    st.info("Por favor, sube un archivo Excel para ver el dashboard.")

# No need for debug prints anymore

import pandas as pd
import numpy as np

# Load and process data from the Excel files
try:
    # Assuming 'NUEVODFEXCEL.xlsx' is the uploaded file for notebook context
    df_notificaciones = pd.read_excel('/content/NUEVODFEXCEL.xlsx')
    df_coordenadas = pd.read_excel('Latitud y longitud.xlsx')

    # Limpieza y preparación de datos de notificaciones
    df_notificaciones.dropna(inplace=True) # Manejar nulos

    columns_to_clean = ['PLAZA/ENTIDAD', 'ZONA', 'MATERIA', 'TURNADO A']
    for col in columns_to_clean:
        df_notificaciones[col] = df_notificaciones[col].astype(str).str.replace(' ', '', regex=False) # Eliminar espacios y asegurar que sean strings

    columns_to_categorize = ['ZONA', 'MATERIA', 'TURNADO A']
    for col in columns_to_categorize:
        df_notificaciones[col] = df_notificaciones[col].astype('category') # Categorizar columnas

    df_notificaciones.columns = df_notificaciones.columns.str.replace(' ', '_') # Renombrar columnas con guion bajo

    # Preparar datos geográficos y combinar
    df_coordenadas = df_coordenadas.rename(columns={'Ciudad, Estado': 'PLAZA/ENTIDAD'}) # Renombrar columna para fusión

    # Asegurar que la columna de fusión en ambos DFs sea string para evitar problemas
    df_notificaciones['PLAZA/ENTIDAD'] = df_notificaciones['PLAZA/ENTIDAD'].astype(str)
    df_coordenadas['PLAZA/ENTIDAD'] = df_coordenadas['PLAZA/ENTIDAD'].astype(str)

    # Apply the same space removal to the merge key in df_coordenadas
    df_coordenadas['PLAZA/ENTIDAD'] = df_coordenadas['PLAZA/ENTIDAD'].str.replace(' ', '', regex=False)

    # Ensure Latitud and Longitud are strings in df_coordenadas before merge and handle potential NaNs
    df_coordenadas['Latitud'] = df_coordenadas['Latitud'].astype(str).fillna('') # Convert to string and fill NaN with empty string
    df_coordenadas['Longitud'] = df_coordenadas['Longitud'].astype(str).fillna('') # Convert to string and fill NaN with empty string

    df_merged = pd.merge(df_notificaciones, df_coordenadas, on='PLAZA/ENTIDAD', how='left')

    # Convertir latitud y longitud a numérico
    def convert_lat_lon(coord_series):
        # Ensure the series is of string type and handle potential NaNs/None explicitly
        coord_series_str = coord_series.astype(str).fillna('')

        # Only process non-empty string values that match the pattern
        valid_coords = coord_series_str[coord_series_str.str.match(r'([\d.]+)°\s*([NSEOW])')]

        if not valid_coords.empty:
            values = valid_coords.str.extract(r'([\d.]+)°\s*([NSEOW])')
            values.columns = ['Value', 'Direction']
            values['Value'] = pd.to_numeric(values['Value'], errors='coerce')
            values['Value'] = np.where(values['Direction'].isin(['O', 'S']), -values['Value'], values['Value'])
            # Reindex the result to match the original series index
            result = pd.Series(np.nan, index=coord_series.index, dtype=float) # Initialize with NaNs
            result.loc[valid_coords.index] = values['Value'] # Assign converted values to matching indices
            return result
        # Return a series of NaNs if no valid coordinates were found or input was empty/all null/non-matching
        return pd.Series(np.nan, index=coord_series.index)

    df_merged['Latitud'] = convert_lat_lon(df_merged['Latitud'])
    df_merged['Longitud'] = convert_lat_lon(df_merged['Longitud'])

    # Eliminar filas con coordenadas nulas after conversion
    df_merged.dropna(subset=['Latitud', 'Longitud'], inplace=True)

    # Convert 'FECHA_RECEPCION' to datetime
    df_merged['FECHA_RECEPCION'] = pd.to_datetime(df_merged['FECHA_RECEPCION'])

    # Add Year and Month columns for filtering (using August and September 2025 for this example)
    df_merged['Year'] = df_merged['FECHA_RECEPCION'].dt.year
    df_merged['Month'] = df_merged['FECHA_RECEPCION'].dt.month_name()

    # Filter for August and September 2025 data
    df_filtered = df_merged[(df_merged['Year'] == 2025) & (df_merged['Month'].isin(['August', 'September']))].copy()


except FileNotFoundError:
    print("Error: Asegúrate de que los archivos de datos existan en las rutas especificadas.")
except Exception as e:
    print(f"An unexpected error occurred during data loading and processing: {e}")

import json

geojson_path = '/content/15-Mex.geojson'

with open(geojson_path, 'r') as f:
    mexico_geojson = json.load(f)

# Display some of the features and properties to understand the structure
if 'features' in mexico_geojson and len(mexico_geojson['features']) > 0:
    print("First feature properties:")
    print(json.dumps(mexico_geojson['features'][0]['properties'], indent=2))
else:
    print("GeoJSON does not contain features or is empty.")

# Display columns in the filtered dataframe to identify a potential join key
print("\nColumns in df_filtered:")
print(df_filtered.columns)

# Display unique values from the 'PLAZA/ENTIDAD' and the extracted 'State' column in df_filtered
# to see if they match anything in the GeoJSON properties.
print("\nUnique values from 'PLAZA/ENTIDAD' in df_filtered:")
print(df_filtered['PLAZA/ENTIDAD'].unique())

# Assuming we previously created a 'State' column in df_filtered for the choropleth attempt
if 'State' in df_filtered.columns:
    print("\nUnique values from 'State' in df_filtered:")
    print(df_filtered['State'].unique())